{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4affc16-8ddc-4e52-93d7-d4c8ec1b84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774a92c-59e6-46d1-a2ce-5733e7207240",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145e83cf-7149-48d8-8491-07f6afe6d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"Dataset/shakesphere.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100f7994-6035-4afa-8d89-16b8d94da845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size : 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f\"Vocabulary : {vocab}\")\n",
    "print(f\"Vocab size : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d05897-9d4a-41d2-8d02-9fdff0012921",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a9ad2a-5354-4d51-a5e0-a9d57d984e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i+1 for i, ch in enumerate(vocab[1:])}\n",
    "itos = {i+1:ch for i, ch in enumerate(vocab[1:])}\n",
    "stoi['\\n'] = 0\n",
    "itos[0] = '\\n'\n",
    "encode = lambda text: [stoi[ch] for ch in text]\n",
    "decode = lambda ix: ''.join([itos[i] for i in ix])\n",
    "text = torch.tensor(encode(text))\n",
    "n1 = int(0.9 * len(text))\n",
    "train_data = text[:n1]\n",
    "val_data = text[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ceb946c-445f-41b9-b514-4845a35b71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(mode):\n",
    "    data = train_data if mode == \"train\" else val_data\n",
    "    batch_size = 4\n",
    "    block_size = 8\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc04048-9fb2-4f51-9e2c-8e73910c23ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bigram Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0eea5334-3f46-4beb-9c7f-9d9b2de3304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, ix, targets = None):\n",
    "        logits = self.token_embedding_table(ix)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, ix, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(ix)\n",
    "            logits = logits[:, -1, :]\n",
    "            p_dis = F.softmax(logits, dim = -1)\n",
    "            next_ix = torch.multinomial(p_dis, num_samples = 1)\n",
    "            ix = torch.cat((ix, next_ix), dim = -1)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "17224126-2a41-4a6d-8989-4a9b43a2a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count : 4225\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "vocab_size = 65\n",
    "model = BigramLanguageModel()\n",
    "print(f\"Parameter count : {sum([p.nelement() for p in model.parameters()])}\")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype = torch.long), 100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a6c7826d-075f-4bfa-a870-f679cb5cca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5648, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4341, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7348, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8343, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6273, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6538, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6867, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4891, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5951, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7333, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.03)\n",
    "for i in range(1000):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%100 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d1419a11-cc35-4229-8d86-26e96f608774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JOHul avere co.\n",
      "CouCKI:\n",
      "HABe y, crd wh tarrisoth\n",
      "Y werariathitoth lll d wnf thye m f fQ's ve PRine f\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype = torch.long), 100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a9792ee4-d880-4364-9b43-abc43c6a061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4673)\n",
      "tensor(2.6564)\n",
      "tensor(2.7202)\n",
      "tensor(2.3033)\n",
      "tensor(2.2906)\n",
      "tensor(2.2546)\n",
      "tensor(2.3078)\n",
      "tensor(2.4898)\n",
      "tensor(2.3975)\n",
      "tensor(2.5163)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        x, y = get_batch(\"eval\")\n",
    "        logits, loss = model(x, y)\n",
    "        if i%100 == 0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98ca67-2bf2-49cf-b267-ddb347c35f6a",
   "metadata": {},
   "source": [
    "# Transformer Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e12ee592-5431-4090-a01b-7c7b1624e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        att_sc = q @ k.transpose(-2, -1)\n",
    "        att_sc = att_sc.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        att_sc = F.softmax(att_sc, dim = -1)\n",
    "        out = att_sc @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_heads\n",
    "        self.sa_head = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa_head(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(4),\n",
    "            Block(4),\n",
    "            Block(4),\n",
    "            Block(4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding_table(x)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, ix, max_new_tokens):\n",
    "        out = ix\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(ix)\n",
    "            logits = logits[:, -1, :]\n",
    "            p_dis = F.softmax(logits, dim = -1)\n",
    "            next_ix = torch.multinomial(p_dis, num_samples = 1)\n",
    "            if ix.shape[1] < block_size:\n",
    "                ix = torch.cat((ix, next_ix), dim = -1)\n",
    "            else:\n",
    "                ix = torch.cat((ix[:, 1:], next_ix), dim = -1)\n",
    "            out = torch.cat((out, next_ix), dim = -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c711797d-b090-4db4-b759-8a163bf5ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fb0f49-ea9f-4593-a755-b7b987fa159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count : 20928\n",
      "\n",
      "iZWcKRDegcgzQamM.FXwHUL$..sLv'vuwajJf Pj$YG;uMWK;$T,H-!VBEK!BshNTIsbhGpkxZ?,BGml!x?oUM-L&r:Q'rMOOHZS\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "vocab_size = 65\n",
    "n_embd = 32\n",
    "\n",
    "model = Transformer()\n",
    "print(f\"Parameter count : {sum([p.nelement() for p in model.parameters()])}\")\n",
    "print(decode(model.generate(torch.zeros(1,1, dtype = torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96cee49-77ee-4ca6-b3b2-93b892019d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0   : train_loss : 4.168072    val_loss : 4.169550\n",
      "step 500   : train_loss : 3.369318    val_loss : 3.369343\n",
      "step 1000   : train_loss : 3.448972    val_loss : 3.504546\n",
      "step 1500   : train_loss : 3.345427    val_loss : 3.357195\n",
      "step 2000   : train_loss : 3.329371    val_loss : 3.383845\n",
      "step 2500   : train_loss : 3.321426    val_loss : 3.392511\n",
      "step 3000   : train_loss : 3.292060    val_loss : 3.382539\n",
      "step 3500   : train_loss : 3.326870    val_loss : 3.355074\n",
      "step 4000   : train_loss : 3.339852    val_loss : 3.324258\n",
      "step 4500   : train_loss : 3.321361    val_loss : 3.367865\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "for i in range(max_iters):\n",
    "    if i%eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}   : train_loss : {losses[\"train\"]:2f}    val_loss : {losses[\"val\"]:2f}\")\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8a7b7871-916a-4dc5-9071-34738ad30383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Juoslc hheh co.\n",
      "CouChn: LAor yi crh wo ca wiroe \n",
      "Ynodraoia,g;t te lll dlen\n",
      "It,ne hon ftoihv E niae f\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "out = decode(model.generate(torch.zeros(1,1, dtype = torch.long), 100)[0].tolist())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b7199b7f-8f75-4e03-ae91-2bad40754295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4404)\n",
      "tensor(2.5505)\n",
      "tensor(2.6389)\n",
      "tensor(2.0694)\n",
      "tensor(2.1598)\n",
      "tensor(2.0800)\n",
      "tensor(2.3222)\n",
      "tensor(2.3897)\n",
      "tensor(2.4044)\n",
      "tensor(2.5176)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss_ls = []\n",
    "    for i in range(1000):\n",
    "        x, y = get_batch(\"eval\")\n",
    "        logits, loss = model(x, y)\n",
    "        loss_ls.append(loss.item())\n",
    "        if i%100 == 0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfb278-db9a-4a7c-91eb-aa505da51e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
